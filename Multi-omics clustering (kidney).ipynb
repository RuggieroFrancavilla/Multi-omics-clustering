{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Multi-omics clustering (kidney).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW5v0Kgdm1nw"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-dGe-kIAZPd"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from itertools import product\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix, adjusted_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError as MAE\n",
        "from tensorflow.keras.regularizers import L1, L2\n",
        "\n",
        "# Import utils.py\n",
        "!gdown 'https://drive.google.com/uc?id=13I5w4WajPg6MObtLPQjxznm8w5hKlEY0' -O ./utils.py\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWxP3HjztXgg"
      },
      "source": [
        "# KIDNEY DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-DTnMNlhPq"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVbjgB0oX9mk"
      },
      "source": [
        "# Load dataset\n",
        "if not os.path.exists(\"./mRNA.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1i1do_UTzwXzPVIDDmYSFJEholK2Mp8g_' -O ./mRNA.txt\n",
        "if not os.path.exists(\"./miRNA.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1liKeOBKjnbCi1CIjcOPA3Zxv2fRzCfa2' -O ./miRNA.txt\n",
        "if not os.path.exists(\"./meth.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1qr9joY0bAVDLvjWsKF5xf3CaRolBu-mP' -O ./meth.txt\n",
        "if not os.path.exists(\"./clusters.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1R-U2iDgM4oEyzNRfBIA2kXMbKw_s0QtI' -O ./clusters.txt\n",
        "\n",
        "ds = {}     # this will contain each omic\n",
        "omics = ['mRNA','miRNA','meth']\n",
        "for omic_name in omics:\n",
        "    path = omic_name + \".txt\"\n",
        "    if omic_name not in ds:\n",
        "        ds[omic_name] = pd.read_csv(path, sep='\\t', index_col=0)\n",
        "\n",
        "y = pd.read_csv('clusters.txt', sep='\\t', index_col=0)\n",
        "true_cluster_labels = y.values.reshape(y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w-E2vaxw4BK"
      },
      "source": [
        "## Preprocess omics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJR_B1kGwlZ3"
      },
      "source": [
        "### mRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzjHdluawkSL"
      },
      "source": [
        "# Keep only protein coding genes\n",
        "# NB: the file idx_mRNA_prot_cod.txt, containing the protein coding genes of the\n",
        "# human genome, has been obtained in the colab notebook 'Find protein coding genes'\n",
        "if not os.path.exists(\"./idx_mRNA_prot_cod_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1Pi4u8y_YAc2tmOWZYaeLn9wGdzu4cFC5' -O ./idx_mRNA_prot_cod_kidney.txt\n",
        "\n",
        "idx_mRNA_prot_cod = pd.read_csv('idx_mRNA_prot_cod_kidney.txt')\n",
        "idx_mRNA_prot_cod = idx_mRNA_prot_cod['idx'].values\n",
        "\n",
        "ds['mRNA'] = ds['mRNA'].iloc[:, idx_mRNA_prot_cod]\n",
        "\n",
        "# Delete genes with a zero expression value across all the samples\n",
        "ds['mRNA'] = ds['mRNA'].loc[:, (ds['mRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize mRNA with MinMax Scaler\n",
        "ds['mRNA_normalized'] = MinMaxScaler().fit_transform(ds['mRNA'].values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYv31QjyxjpO"
      },
      "source": [
        "### miRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7aH-tn7w1zQ"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['miRNA'] = ds['miRNA'].loc[:, (ds['miRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with log2 normalization\n",
        "ds['miRNA'] = np.log(ds['miRNA'] + 1) / np.log(2)\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['miRNA_normalized'] = MinMaxScaler().fit_transform(ds['miRNA'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdalav4yhbe"
      },
      "source": [
        "### meth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ik5RM1Mytf5"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['meth'] = ds['meth'].loc[:, (ds['meth'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['meth_normalized'] = MinMaxScaler().fit_transform(ds['meth'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eIWKU5WDYJP"
      },
      "source": [
        "### Visualize each omic with true cluster labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_7zxDzpv5Bu"
      },
      "source": [
        "Each omic is first reduced to two dimensions with PCA, and then the two principal components are plotted in the 2D plane"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBneatSCre_"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Plot the clustered dataset with true cluster labels\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} visualization')\n",
        "\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzWkfeI9xozX"
      },
      "source": [
        "## Preliminar analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTh6nipb5lEm"
      },
      "source": [
        "### Apply K-means on the original space for each omic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwRAem2xxjp"
      },
      "source": [
        "> K-means on the original space of each omic.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with cluster assignment and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv1ZirjbxFKx"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "\n",
        "    # Apply K-means\n",
        "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "    kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "    plot_2D_dataset(principalComponents, cluster_labels, title=f'{omic} visualization', caption='predicted clusters')\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} visualization', caption='true clusters')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "    # Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "    print(f\"Silhouette, predicted clusters: {silhouette_score(ds[f'{omic}_normalized'], cluster_labels)}\")\n",
        "    print(f\"Silhouette, true clusters: {silhouette_score(ds[f'{omic}_normalized'], true_cluster_labels)}\")\n",
        "    \n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWAcqgVJoXe"
      },
      "source": [
        "### Apply K-means on the reduced space (64 features) for each omic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kymsczg55-pa"
      },
      "source": [
        "> K-means on the reduced space at 64 dimensions space of each omic.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with cluster assignment and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-RLvHj20J3"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "    # Perform dimensionality reduction --> PCA(64)\n",
        "    pca = PCA(64)\n",
        "    ds[f'{omic}_normalized_reduced'] = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Apply K-means\n",
        "    kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(ds[f'{omic}_normalized_reduced'])\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized_reduced'])\n",
        "    kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "    plot_2D_dataset(principalComponents, cluster_labels, title=f'{omic} 64D visualization', caption='predicted clusters')\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} 64D visualization', caption='true clusters')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "    # Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "    print(f\"Silhouette, predicted clusters: {silhouette_score(ds[f'{omic}_normalized_reduced'], cluster_labels)}\")\n",
        "    print(f\"Silhouette, true clusters: {silhouette_score(ds[f'{omic}_normalized_reduced'], true_cluster_labels)}\")\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VauxS62PzkeS"
      },
      "source": [
        "### Apply K-means on the early integrated dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFPX4-7J7B1J"
      },
      "source": [
        "> Concatenation of the dataset for each omic in on early integrated dataset.\n",
        "\n",
        "> K-means on the early integrated dataset.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with cluster assignment and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5zO6VxiTDVO"
      },
      "source": [
        "# Concatenate the omics (early integration)\n",
        "ds['early_integr'] = np.concatenate([ds[f'{omic}_normalized'] for omic in omics], axis=1)\n",
        "\n",
        "for K in range(2,11):\n",
        "    # Apply K-means\n",
        "    kmeans = KMeans(n_clusters=K, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(ds['early_integr'])\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds['early_integr'])\n",
        "    kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "    plot_2D_dataset(principalComponents, cluster_labels, cluster_centers=kmeans.cluster_centers_, title=f'Early integrated dataset visualization', caption=f'{K} predicted clusters')\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'Early integrated dataset visualization', caption='true clusters')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "    # Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "    print(f\"Silhouette, predicted clusters: {silhouette_score(ds['early_integr'], cluster_labels)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAsmXIYyNRTR"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDzjZCQKt3tI"
      },
      "source": [
        "def data_exploration(df):\n",
        "    n_samples, n_features = df.shape\n",
        "    print(f\"##### N. of samples: {n_samples}\")\n",
        "    print(f\"##### N. of features: {n_features}\")\n",
        "    print()\n",
        "\n",
        "    print(\"##### Last 5 samples of the transcriptome:\")\n",
        "    print(df.tail())\n",
        "    print()\n",
        "\n",
        "    print(\"##### Are there any missing values?\")\n",
        "    print(df.isna().sum().any())\n",
        "    print()\n",
        "\n",
        "data_exploration(pd.DataFrame(data= ds['early_integr']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhsTQwk-ke77"
      },
      "source": [
        "### Scree plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSaVBM1S88PS"
      },
      "source": [
        "Scree plot to determine the number of factors to retain principal components to keep in a principal component analysis (PCA)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezZE0XWq2X8O"
      },
      "source": [
        "# PCA\n",
        "def elbow(df):\n",
        "    pca = PCA().fit(df)\n",
        "    height = pca.explained_variance_ratio_*100\n",
        "    y_pos = np.arange(df.values.shape[1])\n",
        "    if df.values.shape[1] < 500:\n",
        "        plt.figure(figsize = (30,5))\n",
        "        plt.bar(y_pos,height)\n",
        "        plt.xticks(y_pos,np.arange(df.values.shape[1]))\n",
        "        plt.ylim(0,110)\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_)*100, 'r-s')\n",
        "        plt.xlabel('number of components')\n",
        "        plt.ylabel('cumulative explained variance')\n",
        "        plt.show()\n",
        "    else:\n",
        "        n_principal_components = 200\n",
        "        plt.figure(figsize = (30,5))\n",
        "        plt.bar(np.arange(n_principal_components),pca.explained_variance_ratio_[:n_principal_components]*100)\n",
        "        plt.xticks(y_pos,np.arange(n_principal_components)) #df.values.shape[1]\n",
        "        plt.ylim(0,110)\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_[:n_principal_components])*100, 'r-s')\n",
        "        plt.xlabel('number of components')\n",
        "        plt.ylabel('cumulative explained variance')\n",
        "        plt.show()\n",
        "        \"\"\"\n",
        "        print('Too many features! Image too big to be shown.')\n",
        "        print('Cumulative sum of explained variance:')\n",
        "        print(np.cumsum(pca.explained_variance_ratio_)*100)\n",
        "        \"\"\"\n",
        "for omic in omics:\n",
        "    print(omic)\n",
        "    elbow(pd.DataFrame(ds[f'{omic}_normalized']))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMrh5dik01g"
      },
      "source": [
        "## Our method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75NW5Qrzp8Ar"
      },
      "source": [
        "### mRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VirxeyNyego1"
      },
      "source": [
        "n_samples, n_features = ds['mRNA'].shape\n",
        "ds['mRNA'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GJwRG6Q6FuJ"
      },
      "source": [
        "Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8irTpq5UZU"
      },
      "source": [
        "n_samples, n_features = ds['mRNA_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['mRNA_normalized']))   # 0,1,...,783; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [16,32]\n",
        "lr_param = [0.0005,0.0001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['mRNA_normalized'][train_index], ds['mRNA_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['mRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(1945, activation='softsign', name='encoder_1')(input_window)   # 2048\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(194, activation='softsign', name='encoder_2')(x)   # 2048\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(194, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(1945, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['mRNA_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded mRNA dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['mRNA_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['mRNA_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded mRNA', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded mRNA', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1Cfr7w8Hyq"
      },
      "source": [
        "BEST_BATCH_SIZE = 16\n",
        "\n",
        "BEST_LR = 0.0005\n",
        "\n",
        "BEST_EPOCH_PHASE_1 =  100\n",
        "\n",
        "BEST_EPOCH_PHASE_2 = 60 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhKNRgUBPkW"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSFSthYc4qX"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR = BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['mRNA_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['mRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "# this is our input placeholder (cioè ogni sample in input deve avere n_features features)\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(1945, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(194, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# TODO vd pag. 2 del pdf: come output dell'encoder lui mette una funzione sigmoide\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(194, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(1945, activation='softsign', name='decoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])  # quando è chiamato autoencoder.predict(), ritorna sia il sample encoded che quello decoded\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['mRNA_encoded'] = encoder(ds['mRNA_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded mRNA dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['mRNA_encoded'] = encoder(ds['mRNA_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['mRNA_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['mRNA_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded mRNA', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded mRNA', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_mRNA_kidney.tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qn2b0l9mXYov"
      },
      "source": [
        "### miRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTihy2o2mgIz"
      },
      "source": [
        "n_samples, n_features = ds['miRNA'].shape\n",
        "ds['miRNA'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rISzIf0grtw"
      },
      "source": [
        "Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0niubYnMg4uz"
      },
      "source": [
        "n_samples, n_features = ds['miRNA_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['miRNA_normalized']))   # 0,1,...,783; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [16,32] # [16,32]\n",
        "lr_param = [0.005,0.001] # [0.005,0.001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['miRNA_normalized'][train_index], ds['miRNA_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['miRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(162, activation='softsign', name='encoder_1')(input_window)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(100, activation='softsign', name='encoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(100, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(162, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['miRNA_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['miRNA_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded miRNA dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['miRNA_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['miRNA_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['miRNA_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded miRNA', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded miRNA', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKYcYXb4j_Kx"
      },
      "source": [
        "BEST_BATCH_SIZE = 16\n",
        "\n",
        "BEST_LR = 0.001\n",
        "\n",
        "BEST_EPOCH_PHASE_1 = 100\n",
        "\n",
        "BEST_EPOCH_PHASE_2 = 60 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR1ztU_8hWkn"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA1-7PrnhMC2"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR = BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['miRNA_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['miRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "# this is our input placeholder (cioè ogni sample in input deve avere n_features features)\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(162, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(100, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(100, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(162, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])  # quando è chiamato autoencoder.predict(), ritorna sia il sample encoded che quello decoded\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['miRNA_encoded'] = encoder(ds['miRNA_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['miRNA_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded miRNA dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['miRNA_encoded'] = encoder(ds['miRNA_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['miRNA_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['miRNA_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['miRNA_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['miRNA_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded miRNA', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded miRNA', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_miRNA_kidney.tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBxJJjnQXbLa"
      },
      "source": [
        "### meth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiWgvsiMmvCc"
      },
      "source": [
        "n_samples, n_features = ds['meth'].shape\n",
        "ds['meth'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoONeNfahdzO"
      },
      "source": [
        "Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S7x483ChddQ"
      },
      "source": [
        "n_samples, n_features = ds['meth_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['meth_normalized']))   # 0,1,...,783; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [16,32]\n",
        "lr_param =[0.0005,0.0001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['meth_normalized'][train_index], ds['meth_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['meth'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)    # cast to float32 (altrimenti non funziona, per un motivo a me ignoto)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(2338, activation='softsign', name='encoder_1')(input_window)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(234, activation='softsign', name='encoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(234, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(2338, activation='softsign', name='decoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['meth_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded meth dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['meth_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['meth_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['meth_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded meth', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded meth', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGqx-dRekC86"
      },
      "source": [
        "BEST_BATCH_SIZE = 16\n",
        "\n",
        "BEST_LR = 0.0001\n",
        "\n",
        "BEST_EPOCH_PHASE_1 = 100\n",
        "\n",
        "BEST_EPOCH_PHASE_2 = 60"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seu0xfQ4hoRt"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLq1nYghpaV"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR =  BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['meth_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['meth'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "# this is our input placeholder (cioè ogni sample in input deve avere n_features features)\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(2338, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(234, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(234, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(2338, activation='softsign', name='decoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])  # quando è chiamato autoencoder.predict(), ritorna sia il sample encoded che quello decoded\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)  # restituisce solo il sample encoded\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['meth_encoded'] = encoder(ds['meth_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded meth dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['meth_encoded'] = encoder(ds['meth_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['meth_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['meth_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['meth_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded meth', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded meth', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_meth_kidney.tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xytQZxZkaeH"
      },
      "source": [
        "## Save the autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH5mA2xDmrAg"
      },
      "source": [
        "# Save autoencoders\n",
        "!zip -r autoencoder_mRNA_kidney.zip autoencoder_mRNA_kidney.tf\n",
        "!zip -r autoencoder_miRNA_kidney.zip autoencoder_miRNA_kidney.tf\n",
        "!zip -r autoencoder_meth_kidney.zip autoencoder_meth_kidney.tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Fxniexs52c"
      },
      "source": [
        "## Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLh9Lfs4q4jW"
      },
      "source": [
        "#####\n",
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds[f'{omic}_encoded'], ds[f'{omic}_encoded']))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "\n",
        "#####\n",
        "# Cluster with spectral clustering\n",
        "avg_dist_matrix_no_weights = np.average(dist_mat, axis=0)\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2_no_weights = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2_no_weights, true_cluster_labels, title='Avg distance matrix visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg without weights integration\n",
        "best_K_no_weights = 0\n",
        "best_silh_no_weights = -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    ##### NO WEIGHTS\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix_no_weights)\n",
        "    silh_no_weights = silhouette_score(avg_dist_matrix_no_weights, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh_no_weights > best_silh_no_weights:\n",
        "        best_silh_no_weights = silh_no_weights\n",
        "        best_K_no_weights = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS, NO WEIGHTS -----\")\n",
        "    print(f\"silhouette: {silh_no_weights}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K_no_weights}')\n",
        "spectral = SpectralClustering(n_clusters=best_K_no_weights, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix_no_weights)\n",
        "plot_2D_dataset(pc2_no_weights, cluster_assignments, title='Avg distance matrix', caption=f'{best_K_no_weights} clusters, silhouette = {best_silh_no_weights:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Avg. distance matrix integration, K={best_K_no_weights}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}