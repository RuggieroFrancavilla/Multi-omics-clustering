{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Multi-omics clustering (synthetic no noise).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW5v0Kgdm1nw"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-dGe-kIAZPd"
      },
      "source": [
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "from itertools import product\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix, adjusted_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError as MAE\n",
        "from tensorflow.keras.regularizers import L1, L2\n",
        "\n",
        "# Import utils.py\n",
        "!gdown 'https://drive.google.com/uc?id=13I5w4WajPg6MObtLPQjxznm8w5hKlEY0' -O ./utils.py\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWxP3HjztXgg"
      },
      "source": [
        "# SYNTHETIC DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-DTnMNlhPq"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVbjgB0oX9mk"
      },
      "source": [
        "# Load dataset\n",
        "if not os.path.exists(\"./mRNA.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12PArkc1RsOm2437mbysxRF4hQMddZOsc' -O ./mRNA.txt\n",
        "if not os.path.exists(\"./meth.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aJkDF0ckxzY4vsnS53s-V89DdAnRVbPo' -O ./meth.txt\n",
        "if not os.path.exists(\"./prot.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iS4u1SZH6r_Dvs7qRSKC444kc_bqmGhJ' -O ./prot.txt\n",
        "if not os.path.exists(\"./clusters.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UtHj4BzBx5hnQkkklJ9ugU5ERjLhjx4W' -O ./clusters.txt\n",
        "\n",
        "ds = {}     # this will contain each omic\n",
        "omics = ['mRNA','meth','prot']\n",
        "for omic_name in omics:\n",
        "    path = omic_name + \".txt\"\n",
        "    ds[omic_name] = pd.read_csv(path, sep='\\t', index_col=0)\n",
        "    ds[omic_name].index.name = None\n",
        "    ds[omic_name] = ds[omic_name].T\n",
        "    # N.B.: the matrices have been transposed so that now we have samples as rows and features as columns\n",
        "\n",
        "\n",
        "y = pd.read_csv('clusters.txt', sep='\\t').set_index('subjects')    # this will contain the true cluster label of each sample\n",
        "true_cluster_labels = y.values.reshape(y.shape[0])-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w-E2vaxw4BK"
      },
      "source": [
        "## Preprocess omics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPE2YLZsszKY"
      },
      "source": [
        "### Normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXO3j0zxs37Y"
      },
      "source": [
        "for omic in omics:\n",
        "    ds[f'{omic}_normalized'] = MinMaxScaler().fit_transform(ds[omic])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eIWKU5WDYJP"
      },
      "source": [
        "### Visualize each omic with true cluster labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_7zxDzpv5Bu"
      },
      "source": [
        "Each omic is first reduced to two dimensions with PCA, and then the two principal components are plotted in the 2D plane"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlBneatSCre_"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Plot the clustered dataset with true cluster labels\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} visualization')\n",
        "\n",
        "    print()\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzWkfeI9xozX"
      },
      "source": [
        "## Preliminar analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTh6nipb5lEm"
      },
      "source": [
        "### Apply K-means on the original space for each omic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwRAem2xxjp"
      },
      "source": [
        "> K-means on the original space of each omic.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with predicted cluster labels and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv1ZirjbxFKx"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "\n",
        "    # Apply K-means\n",
        "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "    kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "    plot_2D_dataset(principalComponents, cluster_labels, title=f'{omic} visualization', caption='predicted clusters')\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} visualization', caption='true clusters')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "    # Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "    print(f\"Silhouette, predicted clusters: {silhouette_score(ds[f'{omic}_normalized'], cluster_labels)}\")\n",
        "    print(f\"Silhouette, true clusters: {silhouette_score(ds[f'{omic}_normalized'], true_cluster_labels)}\")\n",
        "    \n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWAcqgVJoXe"
      },
      "source": [
        "### Apply K-means on the reduced space (64 features) for each omic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kymsczg55-pa"
      },
      "source": [
        "> K-means on the reduced space at 64 dimensions space of each omic.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with cluster assignment and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-RLvHj20J3"
      },
      "source": [
        "for omic in omics:\n",
        "    print(omic)\n",
        "    # Perform dimensionality reduction --> PCA(64)\n",
        "    pca = PCA(64)\n",
        "    ds[f'{omic}_normalized_reduced'] = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "\n",
        "    # Apply K-means\n",
        "    kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(ds[f'{omic}_normalized_reduced'])\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca = PCA(2)\n",
        "    principalComponents = pca.fit_transform(ds[f'{omic}_normalized_reduced'])\n",
        "    kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "    # Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "    plot_2D_dataset(principalComponents, cluster_labels, title=f'{omic} 64D visualization', caption='predicted clusters')\n",
        "    plot_2D_dataset(principalComponents, true_cluster_labels, title=f'{omic} 64D visualization', caption='true clusters')\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "    # Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "    print(f\"Silhouette, predicted clusters: {silhouette_score(ds[f'{omic}_normalized_reduced'], cluster_labels)}\")\n",
        "    print(f\"Silhouette, true clusters: {silhouette_score(ds[f'{omic}_normalized_reduced'], true_cluster_labels)}\")\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VauxS62PzkeS"
      },
      "source": [
        "### Apply K-means on the early integrated dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFPX4-7J7B1J"
      },
      "source": [
        "> Concatenation of the dataset for each omic in on early integrated dataset.\n",
        "\n",
        "> K-means on the early integrated dataset.\n",
        "\n",
        "> Plot the reduced dataset at 2 dimensions with cluster assignment and with true cluster labels.\n",
        "\n",
        "> Evaluation of cluster assignments with the confusion matrix.\n",
        "\n",
        "> Silhouette score computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5zO6VxiTDVO"
      },
      "source": [
        "# Concatenate the omics (early integration)\n",
        "ds['early_integr'] = np.concatenate([ds[f'{omic}_normalized'] for omic in omics], axis=1)\n",
        "\n",
        "# Apply K-means\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(ds['early_integr'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "principalComponents = pca.fit_transform(ds['early_integr'])\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "plot_2D_dataset(principalComponents, cluster_labels, cluster_centers=kmeans.cluster_centers_, title=f'Early integrated dataset visualization', caption='predicted clusters')\n",
        "plot_2D_dataset(principalComponents, true_cluster_labels, title=f'Early integrated dataset visualization', caption='true clusters')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(true_cluster_labels, cluster_labels)\n",
        "\n",
        "# Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "print(f\"Silhouette, predicted clusters: {silhouette_score(ds['early_integr'], cluster_labels)}\")\n",
        "print(f\"Silhouette, true clusters: {silhouette_score(ds['early_integr'], true_cluster_labels)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAsmXIYyNRTR"
      },
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDzjZCQKt3tI"
      },
      "source": [
        "def data_exploration(df):\n",
        "    n_samples, n_features = df.shape\n",
        "    print(f\"##### N. of samples: {n_samples}\")\n",
        "    print(f\"##### N. of features: {n_features}\")\n",
        "    print()\n",
        "\n",
        "    print(\"##### Last 5 samples of the transcriptome:\")\n",
        "    print(df.tail())\n",
        "    print()\n",
        "\n",
        "    print(\"##### Are there any missing values?\")\n",
        "    print(df.isna().sum().any())\n",
        "    print()\n",
        "\n",
        "data_exploration(pd.DataFrame(data= ds['early_integr']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhsTQwk-ke77"
      },
      "source": [
        "### Scree plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSaVBM1S88PS"
      },
      "source": [
        "Scree plot to determine the number of factors to retain principal components to keep in a principal component analysis (PCA)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezZE0XWq2X8O"
      },
      "source": [
        "# PCA\n",
        "def elbow(df):\n",
        "    pca = PCA().fit(df)\n",
        "    height = pca.explained_variance_ratio_*100\n",
        "    y_pos = np.arange(df.values.shape[1])\n",
        "    if df.values.shape[1] < 500:\n",
        "        plt.figure(figsize = (30,5))\n",
        "        plt.bar(y_pos,height)\n",
        "        plt.xticks(y_pos,np.arange(df.values.shape[1]))\n",
        "        plt.ylim(0,110)\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_)*100, 'r-s')\n",
        "        plt.xlabel('number of components')\n",
        "        plt.ylabel('cumulative explained variance')\n",
        "        plt.show()\n",
        "    else:\n",
        "        n_principal_components = 200\n",
        "        plt.figure(figsize = (30,5))\n",
        "        plt.bar(np.arange(n_principal_components),pca.explained_variance_ratio_[:n_principal_components]*100)\n",
        "        plt.xticks(y_pos,np.arange(n_principal_components)) #df.values.shape[1]\n",
        "        plt.ylim(0,110)\n",
        "        plt.plot(np.cumsum(pca.explained_variance_ratio_[:n_principal_components])*100, 'r-s')\n",
        "        plt.xlabel('number of components')\n",
        "        plt.ylabel('cumulative explained variance')\n",
        "        plt.show()\n",
        "        \"\"\"\n",
        "        print('Too many features! Image too big to be shown.')\n",
        "        print('Cumulative sum of explained variance:')\n",
        "        print(np.cumsum(pca.explained_variance_ratio_)*100)\n",
        "        \"\"\"\n",
        "for omic in omics:\n",
        "    print(omic)\n",
        "    elbow(pd.DataFrame(ds[f'{omic}_normalized']))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnMrh5dik01g"
      },
      "source": [
        "## Our method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCL9-r6qFvM9"
      },
      "source": [
        "cluster_assignments_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75NW5Qrzp8Ar"
      },
      "source": [
        "### mRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VirxeyNyego1"
      },
      "source": [
        "n_samples, n_features = ds['mRNA'].shape\n",
        "ds['mRNA'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GJwRG6Q6FuJ"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8irTpq5UZU"
      },
      "source": [
        "n_samples, n_features = ds['mRNA_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['mRNA_normalized']))   # 0,1,...,500; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [32] # [16,32]\n",
        "lr_param = [0.001] # [0.005,0.001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['mRNA_normalized'][train_index], ds['mRNA_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['mRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(512, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['mRNA_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded mRNA dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['mRNA_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['mRNA_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded mRNA', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded mRNA', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEg82DqKKsYp"
      },
      "source": [
        "Store the best params found (to not re-run the grid search every time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkx1xwV8KUnJ"
      },
      "source": [
        "BEST_BATCH_SIZE = 32\n",
        "BEST_LR = 0.001\n",
        "BEST_EPOCH_PHASE_1 = 50\n",
        "BEST_EPOCH_PHASE_2 = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhKNRgUBPkW"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hSFSthYc4qX"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR = BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "#EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['mRNA_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['mRNA'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(512, activation='softsign', name='decoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['mRNA_encoded'] = encoder(ds['mRNA_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['mRNA_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded mRNA dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['mRNA_encoded'] = encoder(ds['mRNA_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['mRNA_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['mRNA_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['mRNA_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['mRNA_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded mRNA', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded mRNA', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_mRNA.tf')\n",
        "# Save the cluster assignments\n",
        "cluster_assignments_dict['mRNA'] = cluster_assignments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBxJJjnQXbLa"
      },
      "source": [
        "### meth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiWgvsiMmvCc"
      },
      "source": [
        "n_samples, n_features = ds['meth'].shape\n",
        "ds['meth'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoONeNfahdzO"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S7x483ChddQ"
      },
      "source": [
        "n_samples, n_features = ds['meth_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['meth_normalized']))   # 0,1,...,500; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [32] # [16,32]\n",
        "lr_param = [0.001] # [0.005,0.001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['meth_normalized'][train_index], ds['meth_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['meth'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(512, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['meth_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded meth dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['meth_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['meth_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['meth_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded meth', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded meth', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mABcSTOmLJ6j"
      },
      "source": [
        "Store the best params found (to not re-run the grid search every time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyGDLaYlLL0t"
      },
      "source": [
        "BEST_BATCH_SIZE = 32\n",
        "BEST_LR = 0.001\n",
        "BEST_EPOCH_PHASE_1 = 50\n",
        "BEST_EPOCH_PHASE_2 = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seu0xfQ4hoRt"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlLq1nYghpaV"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR = BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "#EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['meth_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['meth'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(512, activation='softsign', name='decoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['meth_encoded'] = encoder(ds['meth_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['meth_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['meth_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded meth dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['meth_encoded'] = encoder(ds['meth_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['meth_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['meth_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['meth_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['meth_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded meth', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded meth', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_meth.tf')\n",
        "# Save the cluster assignments\n",
        "cluster_assignments_dict['meth'] = cluster_assignments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MhaIlTiXetl"
      },
      "source": [
        "### prot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntHPzlIonNib"
      },
      "source": [
        "n_samples, n_features = ds['prot'].shape\n",
        "ds['prot'].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XyCixbeikYl"
      },
      "source": [
        "#### Grid search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEECK49yi3KU"
      },
      "source": [
        "n_samples, n_features = ds['prot_normalized'].shape\n",
        "\n",
        "##### Hyperparameters\n",
        "# For the grid search\n",
        "N_SPLITS = 3\n",
        "kf = KFold(n_splits=N_SPLITS)\n",
        "idx = np.arange(len(ds['prot_normalized']))   # 0,1,...,500; indices used for the K-fold splitting of the omic\n",
        "\n",
        "bs_param = [32] # [16,32]\n",
        "lr_param = [0.001] # [0.005,0.001]\n",
        "\n",
        "# For the training\n",
        "EPOCHS_PHASE_1 = 100    # max n. of epochs for the 1st phase (if no early stopping is taken)\n",
        "EPOCHS_PHASE_2 = 60     # max n. of epochs for the 2nd phase (if no early stopping is taken)\n",
        "N_EPOCHS = EPOCHS_PHASE_1 + EPOCHS_PHASE_2  # max n. of epochs (N_EPOCHS are performed if no early stopping is taken)\n",
        "MIN_DELTA = 0.005   # threshold for the early stopping\n",
        "PATIENCE = 30       # threshold for the early stopping\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1          # weight of the closest cluster center loss when it is summed to the reconstruction loss\n",
        "\n",
        "optimal_val_rec_loss = np.inf   # this variable will record the best validation reconstruction loss obtained at the end of the grid search\n",
        "\n",
        "for BATCH_SIZE, LR in cartesian(bs_param, lr_param):\n",
        "    print('Training with:')\n",
        "    print(f'Batch size: {BATCH_SIZE}')\n",
        "    print(f'Learning Rate: {LR}')\n",
        "    EARLY_STOPPING_PHASE_1 = []     # will store the N_SPLITS values obtained for the n. of epochs of phase 1 performed\n",
        "    EARLY_STOPPING_PHASE_2 = []\n",
        "\n",
        "    for train_index, test_index in kf.split(idx):\n",
        "        # N_SPLITS-1 training splits, 1 validation split\n",
        "        X_train, X_test = ds['prot_normalized'][train_index], ds['prot_normalized'][test_index]\n",
        "        Y_train = true_cluster_labels[train_index]\n",
        "\n",
        "        # Shuffle and split the training set in minibatches\n",
        "        tensor_train_ds = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "        temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "        shuffle_ds = temp_ds.shuffle(buffer_size=ds['prot'].shape[0], reshuffle_each_iteration=True)\n",
        "        batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "        # Build the validation set (no batches)\n",
        "        tensor_val_ds = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "\n",
        "        \n",
        "\n",
        "        #####################################################################################\n",
        "        ##############################  MODEL ARCHITECTURE  #################################\n",
        "        #####################################################################################\n",
        "\n",
        "        ##### Autoencoder architecture\n",
        "        input_window = Input(shape=n_features)\n",
        "\n",
        "        # \"encoded\" is the encoded representation of the input\n",
        "        x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "        # \"decoded\" is the lossy reconstruction of the input\n",
        "        x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dense(512, activation='softsign', name='decoder_2')(encoded_layer)\n",
        "        x = BatchNormalization()(x)\n",
        "        decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "        # this model maps an input to its reconstruction and to its encoded representation in the latent space\n",
        "        autoencoder = Model(input_window, outputs = [decoded_layer, encoded_layer])\n",
        "\n",
        "        # this model maps an input to its encoded representation in the latent space\n",
        "        encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "        print(autoencoder.summary())\n",
        "\n",
        "\n",
        "\n",
        "        #####################################################################################\n",
        "        ###################################  TRAINING  ######################################\n",
        "        #####################################################################################\n",
        "        \n",
        "        train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "        train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "        train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "        val_rec_loss = np.zeros(N_EPOCHS)   # mean per-sample reconstruction loss\n",
        "        \n",
        "        optimizer = Adam(learning_rate = LR)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 1 #####\n",
        "\n",
        "        for epoch in range(1, EPOCHS_PHASE_1+1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "            \n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, phase=1)    # mean per-sample validation reconstruction loss\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")           \n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_1.append(epoch) # populate vector of number of epochsof phase 1\n",
        "        last_epoch_phase_1 = epoch  # less than or equal to EPOCHS_PHASE_1\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### Find the n. of clusters with kmeans in the latent space     \n",
        "        ds['prot_encoded'] = encoder(X_train)   # (n_samples, ls_dim)\n",
        "        opt_silhouette = -1 \n",
        "\n",
        "        for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments = kmeans.fit_predict(ds['prot_encoded'])\n",
        "            silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "\n",
        "            # Find the optimal number number of clusters, based on silhouette score\n",
        "            if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "                n_clust_opt = n_clusters\n",
        "                opt_silhouette = silhouette_avg\n",
        "\n",
        "        print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "        n_clusters = n_clust_opt\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "        cluster_assignments = kmeans.fit_predict(ds['prot_encoded'])\n",
        "\n",
        "        # Perform a 2D PCA to visualize the dataset\n",
        "        pca = PCA(2)\n",
        "        pc2 = pca.fit_transform(ds['prot_encoded'])\n",
        "        # Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "        cluster_centers = kmeans.cluster_centers_\n",
        "        # Reduce cluster centers dimensions (for visualization)\n",
        "        kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "        # Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "        plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded prot dataset')\n",
        "        print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "        # Compute silhouette score in the latent space\n",
        "        silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "        print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ##### TRAINING PHASE 2 #####\n",
        "\n",
        "        for epoch in range(last_epoch_phase_1 + 1, last_epoch_phase_1 + EPOCHS_PHASE_2 + 1):  # eventual early stopping\n",
        "            print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, batch in enumerate(batch_ds):\n",
        "\n",
        "                # Open a GradientTape to record the operations run\n",
        "                # during the forward pass, which enables auto-differentiation.\n",
        "                with tf.GradientTape() as tape:\n",
        "\n",
        "                    # Run the forward pass of the layer.\n",
        "                    # The operations that the layer applies\n",
        "                    # to its inputs are going to be recorded\n",
        "                    # on the GradientTape.\n",
        "                    decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "                    # Compute the loss value for this minibatch.\n",
        "                    # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "                    loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "                    \n",
        "                    train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "                    train_rec_loss[epoch-1][step] = l1\n",
        "                    train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "            # The autoencoder is now updated\n",
        "            # Update the K-means cluster assignments and cluster centers\n",
        "            ds['prot_encoded'] = encoder(X_train)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "            cluster_assignments_new = kmeans.fit_predict(ds['prot_encoded'])\n",
        "            cluster_centers_new = kmeans.cluster_centers_\n",
        "\n",
        "            # Evaluate the silhouette w.r.t. the new cluster assignments:\n",
        "            # if the encoded dataset is more separable with these assignments,\n",
        "            # cluster assignment and cluster centers are updated, otherwise they remain unchanged\n",
        "            if silhouette_avg < silhouette_score(ds['prot_encoded'], cluster_assignments_new):\n",
        "                cluster_assignments = cluster_assignments_new\n",
        "                cluster_centers = cluster_centers_new\n",
        "                print(\"Cluster centers have changed\")\n",
        "                    \n",
        "            print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "            print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "            print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "            \n",
        "            silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "            print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "            \n",
        "            # Perform a 2D PCA to visualize the dataset\n",
        "            pca = PCA(2)\n",
        "            pc2 = pca.fit_transform(ds['prot_encoded'])\n",
        "            # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "            plot_2D_dataset(pc2, cluster_assignments, title='Encoded prot', caption='predicted clusters')\n",
        "            plot_2D_dataset(pc2, Y_train, title='Encoded prot', caption='true clusters')\n",
        "            # Plot confusion matrix\n",
        "            plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "\n",
        "            # End of the epoch: VALIDATION STEP\n",
        "            decoded_val_ds, _ = autoencoder(tensor_val_ds, training=False)  # Logits for this minibatch (in the latent space)\n",
        "            loss_value_val = custom_loss(tensor_val_ds, decoded_val_ds, encoded_batch=None, phase=1)\n",
        "            val_rec_loss[epoch-1] = tf.cast(loss_value_val, tf.float32)\n",
        "            print(\"Mean validation loss for epoch %d = %.4f\" % (epoch, val_rec_loss[epoch-1]))\n",
        "\n",
        "            # Check if early stopping conditions are met\n",
        "            stopEarly = Callback_EarlyStopping(val_rec_loss[EPOCHS_PHASE_1:epoch-1], min_delta=MIN_DELTA, patience=PATIENCE)\n",
        "            if stopEarly:\n",
        "                print(\"Callback_EarlyStopping signal received at epoch= %d/%d\"%(epoch,EPOCHS_PHASE_1))\n",
        "                print(\"Terminating training\")\n",
        "                break\n",
        "        EARLY_STOPPING_PHASE_2.append(epoch - last_epoch_phase_1)   # populate vector of number of epochs of phase 2\n",
        "    \n",
        "    # Remove the zero values which have not been modified (because of early stopping)\n",
        "    train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "    train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "    train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "    val_rec_loss = val_rec_loss[:epoch-1]\n",
        "\n",
        "    plot_confusion_matrix(Y_train, cluster_assignments)\n",
        "\n",
        "    # Evolution of training and validation losses\n",
        "    plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "    plt.title('Train loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "    plt.title('Train reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "    plt.title('Train closest cluster center loss')\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(1,len(val_rec_loss)+1), val_rec_loss)\n",
        "    plt.title('Validation reconstruction loss')\n",
        "    plt.show()\n",
        "\n",
        "    # For the grid search: are the current hyperparameters better than the others tested so far?\n",
        "    if val_rec_loss[-1] < optimal_val_rec_loss:\n",
        "        optimal_val_rec_loss = val_rec_loss[-1]\n",
        "        BEST_BATCH_SIZE = BATCH_SIZE\n",
        "        BEST_LR = LR\n",
        "        BEST_EPOCH_PHASE_1 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_1))) # mean value of the epochs of the 1st phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "        BEST_EPOCH_PHASE_2 = int(np.trunc(np.mean(EARLY_STOPPING_PHASE_2))) # mean value of the epochs of the 2nd phase (the average is across the N_SPLITS values obtained during cross-val)\n",
        "\n",
        "\n",
        "print(f'\\nBest params found: Batch size: {BEST_BATCH_SIZE}, Learning Rate: {BEST_LR}, Epochs 1st phase: {BEST_EPOCH_PHASE_1}, Epochs 2nd phase: {BEST_EPOCH_PHASE_2}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Dp09fRMq_d"
      },
      "source": [
        "Store the best params found (to not re-run the grid search every time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0FC8bekMv_k"
      },
      "source": [
        "BEST_BATCH_SIZE = 32\n",
        "BEST_LR = 0.001\n",
        "BEST_EPOCH_PHASE_1 = 50\n",
        "BEST_EPOCH_PHASE_2 = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilk0k462imIj"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLgj1gn5im9D"
      },
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = BEST_BATCH_SIZE\n",
        "LR = BEST_LR\n",
        "N_EPOCHS = BEST_EPOCH_PHASE_1 + BEST_EPOCH_PHASE_2\n",
        "#EPOCHS_PHASE_1 = BEST_EPOCH_PHASE_1\n",
        "MAX_CLUSTERS = 10   # max n. of clusters to search for at the end of the 1st phase\n",
        "LMBD = 0.1 # 0.1\n",
        "\n",
        "# Shuffle and split the dataset in minibatches\n",
        "tensor_train_ds = tf.convert_to_tensor(ds['prot_normalized'], dtype=tf.float32)\n",
        "temp_ds = Dataset.from_tensor_slices(tensor_train_ds)\n",
        "shuffle_ds = temp_ds.shuffle(buffer_size=ds['prot'].shape[0], reshuffle_each_iteration=True)\n",
        "batch_ds = shuffle_ds.batch(BATCH_SIZE)\n",
        "\n",
        "#####################################################################################\n",
        "##############################  MODEL ARCHITECTURE  #################################\n",
        "#####################################################################################\n",
        "\n",
        "##### Autoencoder architecture\n",
        "input_window = Input(shape=n_features)\n",
        "\n",
        "# \"encoded\" is the encoded representation of the input\n",
        "x = Dense(512, activation='softsign', name='encoder_1')(input_window)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(256, activation='softsign', name='encoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "encoded_layer = Dense(64, activation='sigmoid', name='encoder_3')(x)\n",
        "\n",
        "# \"decoded\" is the lossy reconstruction of the input\n",
        "x = Dense(256, activation='softsign', name='decoder_1')(encoded_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(512, activation='softsign', name='decoder_2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "decoded_layer = Dense(n_features, activation='sigmoid', name='decoder_3')(x)\n",
        "\n",
        "# this model maps an input to its reconstruction\n",
        "autoencoder = Model(input_window, outputs = [decoded_layer,encoded_layer])\n",
        "\n",
        "# this model maps an input to its encoded representation in the latent space\n",
        "encoder = Model(input_window, encoded_layer)\n",
        "\n",
        "print(autoencoder.summary())\n",
        "\n",
        "\n",
        "#####################################################################################\n",
        "###################################  TRAINING  ######################################\n",
        "#####################################################################################\n",
        "\n",
        "train_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample loss (the mean is taken for each minibatch)\n",
        "train_rec_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample reconstruction loss\n",
        "train_ccc_loss = np.zeros((N_EPOCHS, len(batch_ds)))    # mean per-sample closest cluster center loss\n",
        "\n",
        "optimizer = Adam(LR)\n",
        "\n",
        "\n",
        "\n",
        "##### TRAINING PHASE 1 #####\n",
        "for epoch in range(1, BEST_EPOCH_PHASE_1+1): \n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (nel latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch\n",
        "            loss_value = custom_loss(batch, decoded_batch, phase=1)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "##### END OF TRAINING PHASE 1 #####\n",
        "        \n",
        "\n",
        "\n",
        "##### Find the n. of clusters with kmeans in the latent space\n",
        "ds['prot_encoded'] = encoder(ds['prot_normalized'])   # (n_samples, ls_dim)\n",
        "opt_silhouette = -1 \n",
        "\n",
        "for n_clusters in np.arange(2, MAX_CLUSTERS+1):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments = kmeans.fit_predict(ds['prot_encoded'])\n",
        "    silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "\n",
        "    # Find the optimal number number of clusters, based on silhouette score\n",
        "    if silhouette_avg >= opt_silhouette:    # better n. of clusters found\n",
        "        n_clust_opt = n_clusters\n",
        "        opt_silhouette = silhouette_avg\n",
        "\n",
        "print(f\"The best number of clusters found is: {n_clust_opt}\")\n",
        "n_clusters = n_clust_opt\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "cluster_assignments = kmeans.fit_predict(ds['prot_encoded'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "pc2 = pca.fit_transform(ds['prot_encoded'])\n",
        "# Cluster centers in the latent space (needed for computing the loss in phase 2)\n",
        "cluster_centers = kmeans.cluster_centers_\n",
        "# Reduce cluster centers dimensions (for visualization)\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Plot 2D visualization of the encoded dataset with the cluster assignments found\n",
        "plot_2D_dataset(pc2, cluster_assignments, cluster_centers=kmeans.cluster_centers_, title='Encoded prot dataset')\n",
        "print(f\"Number of clusters: {cluster_centers.shape[0]}\")\n",
        "\n",
        "# Compute silhouette score in the latent space\n",
        "silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "\n",
        "       \n",
        "##### TRAINING PHASE 2 #####\n",
        "for epoch in range(BEST_EPOCH_PHASE_1 + 1, N_EPOCHS + 1):  # eventual early stopping\n",
        "    print(f\"\\nStart of epoch {epoch}\")\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, batch in enumerate(batch_ds):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            decoded_batch, encoded_batch = autoencoder(batch, training=True)  # Logits for this minibatch (in the latent space)\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            # To compute this loss we need the cluster centers of the previous step (t-1)\n",
        "\n",
        "            loss_value, l1, l2 = custom_loss(batch, decoded_batch, encoded_batch, cluster_centers, lmbd=LMBD, phase=2)\n",
        "            \n",
        "            train_loss[epoch-1][step] = tf.cast(loss_value, tf.float32)\n",
        "            train_rec_loss[epoch-1][step] = l1\n",
        "            train_ccc_loss[epoch-1][step] = l2\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, autoencoder.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "\n",
        "\n",
        "    ds['prot_encoded'] = encoder(ds['prot_normalized']) \n",
        "\n",
        "    # Perform KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
        "    cluster_assignments_new = kmeans.fit_predict(ds['prot_encoded'])\n",
        "    cluster_centers_new = np.copy(kmeans.cluster_centers_)\n",
        "\n",
        "    if silhouette_avg < silhouette_score(ds['prot_encoded'], cluster_assignments_new):\n",
        "        cluster_assignments = cluster_assignments_new\n",
        "        cluster_centers = cluster_centers_new\n",
        "        print(\"Cluster centers have changed\")\n",
        "            \n",
        "    print(\"Mean training loss for epoch %d = %.4f\" % (epoch, train_loss[epoch-1].mean()))\n",
        "    print(\"Mean training reconstruction loss for epoch %d = %.4f\" % (epoch, train_rec_loss[epoch-1].mean()))\n",
        "    print(\"Mean training closest cluster loss for epoch %d = %.4f\" % (epoch, train_ccc_loss[epoch-1].mean()))\n",
        "                \n",
        "    silhouette_avg = silhouette_score(ds['prot_encoded'], cluster_assignments)\n",
        "    print(f\"Silhouette on the encoded dataset: {silhouette_avg}\")\n",
        "\n",
        "    # Perform a 2D PCA to visualize the dataset\n",
        "    pca2 = PCA(2)\n",
        "    pc2 = pca2.fit_transform(ds['prot_encoded'])\n",
        "    # Plot the encoded space in 2D with predicted clusters and true cluster labels\n",
        "    plot_2D_dataset(pc2, cluster_assignments, title='Encoded prot', caption='predicted clusters')\n",
        "    plot_2D_dataset(pc2, true_cluster_labels, title='Encoded prot', caption='true clusters')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(true_cluster_labels, cluster_assignments)\n",
        "##### END OF TRAINING PHASE 2 #####\n",
        "\n",
        "train_loss = train_loss.mean(axis=1)[:epoch-1]\n",
        "train_rec_loss = train_rec_loss.mean(axis=1)[:epoch-1]\n",
        "train_ccc_loss = train_ccc_loss.mean(axis=1)[:epoch-1]\n",
        "\n",
        "# Evolution of training and validation losses\n",
        "plt.plot(np.arange(1,len(train_loss)+1), train_loss)\n",
        "plt.title('Train loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_rec_loss)+1), train_rec_loss)\n",
        "plt.title('Train reconstruction loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(np.arange(1,len(train_ccc_loss)+1), train_ccc_loss)\n",
        "plt.title('Train closest cluster center loss')\n",
        "plt.show()\n",
        "\n",
        "# Save the autoencoder\n",
        "autoencoder.save('autoencoder_prot.tf')\n",
        "# Save the cluster assignments\n",
        "cluster_assignments_dict['prot'] = cluster_assignments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xytQZxZkaeH"
      },
      "source": [
        "## Save the autoencoders / load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH5mA2xDmrAg"
      },
      "source": [
        "# Save autoencoders\n",
        "!zip -r autoencoder_mRNA_synthetic.zip autoencoder_mRNA.tf\n",
        "!zip -r autoencoder_meth_synthetic.zip autoencoder_meth.tf\n",
        "!zip -r autoencoder_prot_synthetic.zip autoencoder_prot.tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Fxniexs52c"
      },
      "source": [
        "## Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoGISrBNBCOn"
      },
      "source": [
        "# Put together the cluster assignments resulting from each AE's latent space\n",
        "cluster_assignments_mat = np.array([cluster_assignments_dict[f'{omic}'] for omic in omics])\n",
        "\n",
        "n_omics = len(omics)    # 3\n",
        "n_patients = cluster_assignments_mat.shape[1]   # 500\n",
        "\n",
        "#####\n",
        "#For the weighted average method: compute the weights to use in the average\n",
        "weights = np.zeros((n_omics, n_patients, n_patients))\n",
        "\n",
        "for p1 in range(n_patients):    # for each patient p1\n",
        "    clustered_together = []\n",
        "    for o in range(len(omics)): # for all the omics\n",
        "        # get a bool vector of patients clustered together with p1, w.r.t. current omic\n",
        "        # True where same cluster assignment of p1, False otherwise\n",
        "        clustered_together.append( cluster_assignments_mat[o, :] == cluster_assignments_mat[o, :][p1] )\n",
        "    clustered_together = np.array(clustered_together)  # (3, 500); clustered_together contains obviously a vector for each omic\n",
        "\n",
        "    for p2 in range(p1, n_patients):    # for each patient p2\n",
        "        # find the omics in which p1 and p2 are clustered together\n",
        "        omics_mask = clustered_together[:, p2]  # (3)\n",
        "        count_together = np.sum(omics_mask)  # n. of omics in which p1 and p2 are clustered together\n",
        "        count_not_together = len(omics) - count_together    # n. of omics in which p1 and p2 are NOT clustered together\n",
        "\n",
        "        # set the weights to count_together where omics_mask == True, count_not_together otherwise\n",
        "        weights[omics_mask, p1, p2] = count_together\n",
        "        weights[np.invert(omics_mask), p1, p2] = count_not_together\n",
        "\n",
        "# populate the lower-left triangle of weights (which is still 0)\n",
        "# this can be done because each slice of weights matrix must obviously be symmetric\n",
        "for o in range(len(omics)):\n",
        "    weights[o,:,:] = weights[o,:,:] + weights[o,:,:].T - np.diag(np.diag(weights[o,:,:]))\n",
        "\n",
        "\n",
        "#####\n",
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds[f'{omic}_encoded'], ds[f'{omic}_encoded']))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "\n",
        "#####\n",
        "# Cluster with spectral clustering\n",
        "avg_dist_matrix_no_weights = np.average(dist_mat, axis=0)\n",
        "avg_dist_matrix_weights = np.average(dist_mat, axis=0, weights=weights)\n",
        "\n",
        "pc2_no_weights = PCA(2).fit_transform(avg_dist_matrix_no_weights)\n",
        "plot_2D_dataset(pc2_no_weights, true_cluster_labels, title='Avg distance matrix visualization', caption='True labels')\n",
        "\n",
        "pc2_weights = PCA(2).fit_transform(avg_dist_matrix_weights)\n",
        "plot_2D_dataset(pc2_weights, true_cluster_labels, title='Weighted avg distance matrix visualization', caption='True labels')\n",
        "\n",
        "\n",
        "\n",
        "#####\n",
        "# Results for the avg without weights integration\n",
        "best_K_no_weights = 0\n",
        "best_silh_no_weights = -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    ##### NO WEIGHTS\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix_no_weights)\n",
        "    silh_no_weights = silhouette_score(avg_dist_matrix_no_weights, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh_no_weights > best_silh_no_weights:\n",
        "        best_silh_no_weights = silh_no_weights\n",
        "        best_K_no_weights = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS, NO WEIGHTS -----\")\n",
        "    print(f\"silhouette: {silh_no_weights}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K_no_weights}')\n",
        "spectral = SpectralClustering(n_clusters=best_K_no_weights, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix_no_weights)\n",
        "plot_2D_dataset(pc2_no_weights, cluster_assignments, title='Avg distance matrix', caption=f'{K} clusters, silhouette = {best_silh_no_weights:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Avg. distance matrix integration, K={best_K_no_weights}')\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "#####\n",
        "# Results for the avg without weights integration\n",
        "best_K_weights = 0\n",
        "best_silh_weights = -1\n",
        "\n",
        "\n",
        "#####\n",
        "# Results for the avg with weights integration\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    ##### WEIGHTS\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix_weights)\n",
        "    silh_weights = silhouette_score(avg_dist_matrix_weights, cluster_assignments,metric = \"precomputed\")\n",
        "    if silh_weights > best_silh_weights:\n",
        "        best_silh_weights = silh_weights\n",
        "        best_K_weights = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS, WEIGHTS -----\")\n",
        "    print(f\"silhouette: {silh_weights}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (weighted avg integration): {best_K_weights}')\n",
        "spectral = SpectralClustering(n_clusters=best_K_weights, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix_weights)\n",
        "plot_2D_dataset(pc2_weights, cluster_assignments, title='Weighted avg distance matrix', caption=f'{K} clusters, silhouette = {best_silh_weights:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Weighted avg. distance matrix integration, K={best_K_weights}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKJghxtyid05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
