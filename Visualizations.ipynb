{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di Visualizations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Sju3Zp-wSvF"
      },
      "source": [
        "# Import utils.py\n",
        "!gdown 'https://drive.google.com/uc?id=13I5w4WajPg6MObtLPQjxznm8w5hKlEY0' -O ./utils.py\n",
        "from utils import *\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix, adjusted_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDACljw_jupz"
      },
      "source": [
        "# Synthetic dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcYJZrNbjuxG"
      },
      "source": [
        "# Upload on Google Colab, autoencoder for each omic \n",
        "ds_ae = {}     # this will contain each autoencoder trained on each omic\n",
        "# Load autoencoders\n",
        "!gdown 'https://drive.google.com/uc?id=1D_4E3muRg4ULC-0GQJRa8-XEcOwCFpUM' -O ./autoencoder_mRNA_synthetic.zip\n",
        "!gdown 'https://drive.google.com/uc?id=1PGgUGB-_qQ44XBV6WlSkZanKRqXKn_zD' -O ./autoencoder_meth_synthetic.zip\n",
        "!gdown 'https://drive.google.com/uc?id=1BZqv3_2qqBa4nPWT8RIGg82k-zKwvOeM' -O ./autoencoder_prot_synthetic.zip\n",
        "\n",
        "!unzip autoencoder_mRNA_synthetic.zip\n",
        "ds_ae[\"mRNA\"] = tf.keras.models.load_model('autoencoder_mRNA_synthetic.tf')\n",
        "!unzip autoencoder_meth_synthetic.zip\n",
        "ds_ae[\"meth\"] = tf.keras.models.load_model('autoencoder_meth_synthetic.tf')\n",
        "!unzip autoencoder_prot_synthetic.zip\n",
        "ds_ae[\"prot\"] = tf.keras.models.load_model('autoencoder_prot_synthetic.tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IGu5LbrmMrF"
      },
      "source": [
        "# Load dataset\n",
        "if not os.path.exists(\"./mRNA_synthetic.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12PArkc1RsOm2437mbysxRF4hQMddZOsc' -O ./mRNA_synthetic.txt\n",
        "if not os.path.exists(\"./meth_synthetic.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1aJkDF0ckxzY4vsnS53s-V89DdAnRVbPo' -O ./meth_synthetic.txt\n",
        "if not os.path.exists(\"./prot_synthetic.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1iS4u1SZH6r_Dvs7qRSKC444kc_bqmGhJ' -O ./prot_synthetic.txt\n",
        "if not os.path.exists(\"./clusters_synthetic.txt\"):\n",
        "    !wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1UtHj4BzBx5hnQkkklJ9ugU5ERjLhjx4W' -O ./clusters_synthetic.txt\n",
        "\n",
        "ds = {}     # this will contain each omic\n",
        "omics = ['mRNA','meth','prot']\n",
        "for omic_name in omics:\n",
        "    path = omic_name + \"_synthetic.txt\"\n",
        "    ds[omic_name] = pd.read_csv(path, sep='\\t').drop(columns=[\"probe\"]).T  \n",
        "\n",
        "\n",
        "for omic in omics:\n",
        "    if omic == \"meth\":\n",
        "        mu = 0 \n",
        "        sigma = 0.4 \n",
        "\n",
        "    elif omic == \"prot\":\n",
        "        mu = 0\n",
        "        sigma = 4\n",
        "\n",
        "    else:\n",
        "        mu = 0\n",
        "        sigma = 4\n",
        "        \n",
        "    n_samples, n_features = ds[omic].shape\n",
        "\n",
        "    # Add salt & pepper noise\n",
        "    np.random.seed(42)   # fixed seed for reproducibility\n",
        "    noise = np.random.normal(mu, sigma, size=(n_samples, n_features))\n",
        "    p = 0.5 #0.8     # salt & pepper noise: add (gaussian) noise to a particolar feature of a particular sample with probability p\n",
        "    raveled_indices = np.random.choice(np.arange(n_samples*n_features), replace=False, size=int(n_samples*n_features*(1-p)))\n",
        "    indices = np.unravel_index(raveled_indices, (n_samples, n_features))\n",
        "    noise[indices] = 0\n",
        "\n",
        "    print(f\"Salt & pepper gaussian noise N({mu},{sigma**2}) is added to the {omic} dataset\")\n",
        "    ds[f'{omic}_noisy'] = ds[f'{omic}'] + noise\n",
        "\n",
        "    # Normalize omic\n",
        "    ds[f'{omic}_noisy'] = MinMaxScaler().fit_transform(ds[f'{omic}_noisy'])\n",
        "\n",
        "\n",
        "y = pd.read_csv('clusters_synthetic.txt', sep='\\t')    # this will contain the cluster assignment of each sample\n",
        "y = y.reset_index().drop(labels='index', axis=1).set_index('subjects')\n",
        "y.index.name = None\n",
        "true_cluster_labels = y.values.reshape(y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMnsR-2KpqzC"
      },
      "source": [
        "We can verify that despite introducing an even higher noise, our methods still performs well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cdvDkWAVbqN"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGX2IUlK_9R"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_noisy'],)\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C9QjlLRWT6a"
      },
      "source": [
        "Early integration + K-means (For comparison)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XVlgeKNWT6a"
      },
      "source": [
        "# Concatenate the omics (early integration)\n",
        "ds['early_integr'] = np.concatenate([ds[f'{omic}_noisy'] for omic in omics], axis=1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(ds['early_integr'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "principalComponents = pca.fit_transform(ds['early_integr'])\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "print(f\"Silhouette, 5 clusters: {silhouette_score(ds['early_integr'], cluster_labels)}\")\n",
        "\n",
        "# Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "plot_2D_dataset(principalComponents, cluster_labels, cluster_centers=kmeans.cluster_centers_, title=f'Early integrated dataset visualization', caption=f'5 clusters')\n",
        "plot_2D_dataset(principalComponents, true_cluster_labels, title=f'Early integrated dataset visualization', caption='true clusters')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(true_cluster_labels, cluster_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M04-LgrJ77V"
      },
      "source": [
        "## Integration method (w/ noise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-swRHGyJ77V"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_noisy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ihRJtCDJ77W"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt0WGwLMJ77W"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels-1,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlkdP4hXnsw7"
      },
      "source": [
        "# Synthetic dataset (no noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2Ei1zT0K1c-"
      },
      "source": [
        "## Integration method (no noise)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez_hiD6jMt53"
      },
      "source": [
        "for omic in omics:\n",
        "    # Normalize omic\n",
        "    ds[f'{omic}_normalized'] = MinMaxScaler().fit_transform(ds[f'{omic}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHWy0yNMVV-9"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQqm--_7M0hG"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_normalized'],)\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVScZ6BCK1dP"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_normalized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HN4PbipK1dR"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HVX6PMdK1dR"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels-1,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxpvAA3Bjtsy"
      },
      "source": [
        "# Lung dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAvUkxXf_l3"
      },
      "source": [
        "# Upload on Google Colab, autoencoder for each omic \n",
        "ds_ae = {}     # this will contain each autoencoder trained on each omic\n",
        "\n",
        "# Load autoencoders\n",
        "if not os.path.exists(\"./autoencoder_mRNA_lung.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=17pz12qMXWmoKwjmQGRwqGDMc8BzBZzAT' -O ./autoencoder_mRNA_lung.zip\n",
        "if not os.path.exists(\"./autoencoder_miRNA_lung.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=16K2hzgnPvqd3bRHAzE_Pkj3Js23fIYlU' -O ./autoencoder_miRNA_lung.zip\n",
        "if not os.path.exists(\"./autoencoder_meth_lung.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1QEEiARWDeANMZ_Uoj2zHexz74eLNj2K8' -O ./autoencoder_meth_lung.zip\n",
        "if not os.path.exists(\"./autoencoder_cnv_lung.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1hgAqYL8fUJaYsAL5Gpx10_zNaRqqlsKO' -O ./autoencoder_cnv_lung.zip\n",
        "\n",
        "!unzip autoencoder_mRNA_lung.zip\n",
        "ds_ae[\"mRNA\"] = tf.keras.models.load_model('autoencoder_mRNA_lung.tf')\n",
        "!unzip autoencoder_miRNA_lung.zip\n",
        "ds_ae[\"miRNA\"] = tf.keras.models.load_model('autoencoder_miRNA_lung.tf')\n",
        "!unzip autoencoder_meth_lung.zip\n",
        "ds_ae[\"meth\"] = tf.keras.models.load_model('autoencoder_meth_lung.tf')\n",
        "!unzip autoencoder_cnv_lung.zip\n",
        "ds_ae[\"cnv\"] = tf.keras.models.load_model('autoencoder_cnv_lung.tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQPSY6atjujL"
      },
      "source": [
        "# Load dataset\n",
        "if not os.path.exists(\"./mRNA_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1kNQxRoCs6TIGVzdlpEpUkAqx8FyoSrsi' -O ./mRNA_lung.txt\n",
        "if not os.path.exists(\"./miRNA_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=16V4tY8GeCUemOrf_KBqJwdixrQYZmFHv' -O ./miRNA_lung.txt\n",
        "if not os.path.exists(\"./meth_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1UuCNcXbxHS1lW3bRkixsbo77xIFsWxIG' -O ./meth_lung.txt\n",
        "if not os.path.exists(\"./cnv_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1RT1EiQCj19gGD8bYr0UBjxvUPl8473FX' -O ./cnv_lung.txt  # corrected for batch effects\n",
        "if not os.path.exists(\"./clusters_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1t1rTrZNVHqZRo_F0YA0heD3WbvijHox7' -O ./clusters_lung.txt\n",
        "\n",
        "ds = {}     # this will contain each omic\n",
        "omics = ['mRNA','miRNA','meth','cnv']\n",
        "for omic_name in omics:\n",
        "    path = omic_name + \"_lung.txt\"\n",
        "    if omic_name not in ds:\n",
        "        ds[omic_name] = pd.read_csv(path, sep='\\t', index_col=0)\n",
        "\n",
        "y = pd.read_csv('clusters_lung.txt', sep='\\t', index_col=0)\n",
        "true_cluster_labels = y.values.reshape(y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y18zzK68bD2H"
      },
      "source": [
        "### mRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2IJCV6YbD2H"
      },
      "source": [
        "# Keep only protein coding genes\n",
        "# NB: the file idx_mRNA_prot_cod.txt, containing the protein coding genes of the\n",
        "# human genome, has been obtained in the colab notebook 'Find protein coding genes'\n",
        "if not os.path.exists(\"./idx_mRNA_prot_cod_lung.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1i6lXPIh60BtCF-ujfaqf4WrvWM4XMvNF' -O ./idx_mRNA_prot_cod_lung.txt\n",
        "\n",
        "idx_mRNA_prot_cod = pd.read_csv('idx_mRNA_prot_cod_lung.txt')\n",
        "idx_mRNA_prot_cod = idx_mRNA_prot_cod['idx'].values\n",
        "\n",
        "ds['mRNA'] = ds['mRNA'].iloc[:, idx_mRNA_prot_cod]\n",
        "\n",
        "# Delete genes with a zero expression value across all the samples\n",
        "ds['mRNA'] = ds['mRNA'].loc[:, (ds['mRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize mRNA with MinMax Scaler\n",
        "ds['mRNA_normalized'] = MinMaxScaler().fit_transform(ds['mRNA'].values)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_RtBhdvbD2H"
      },
      "source": [
        "### miRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRV990FobD2I"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['miRNA'] = ds['miRNA'].loc[:, (ds['miRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with log2 normalization\n",
        "ds['miRNA'] = np.log(ds['miRNA'] + 1) / np.log(2)\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['miRNA_normalized'] = MinMaxScaler().fit_transform(ds['miRNA'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dytdqJPlbD2I"
      },
      "source": [
        "### meth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHvb03hPbD2I"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['meth'] = ds['meth'].loc[:, (ds['meth'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['meth_normalized'] = MinMaxScaler().fit_transform(ds['meth'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTL1Fx_Z0-Xd"
      },
      "source": [
        "### cnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VxkoMbTi6Ar"
      },
      "source": [
        "N.B.: batch effect correction has been performed on this omic (see 'Lung dataset retrieval' colab notebook);\n",
        "\n",
        "Notice that, before applying the correction, as a pre-processing step we filtered out duplicates of each column, so to keep only one of each duplicate. This filters out a lot of genes.\n",
        "\n",
        "The rationale behind this is that removing those columns preserves the distances between samples, while greatly reducing the dimensionality of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlKduzMl0-vB"
      },
      "source": [
        "# Normalize with MinMaxScaler\n",
        "ds['cnv_normalized'] = MinMaxScaler().fit_transform(ds['cnv'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7jqXow2VihU"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdx3EmcmM6X9"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_normalized'],)\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkfDUotCJ6en"
      },
      "source": [
        "## Integration method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_szbZ08eJ6eo"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_normalized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvVSexGqJ6eo"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFFcUCspJ6eo"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbCJYZgbOK2S"
      },
      "source": [
        "# Lung dataset w/ noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUUNCc17pyiH"
      },
      "source": [
        "We can verify that when introducing some salt & pepper noise, our method still performs well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkRalziOJxS"
      },
      "source": [
        "for omic in omics:\n",
        "    if omic == \"mRNA\":\n",
        "        mu = 0 \n",
        "        sigma = 4 \n",
        "\n",
        "    elif omic == \"miRNA\":\n",
        "        mu = 0\n",
        "        sigma = 5\n",
        "\n",
        "    elif omic == \"meth\":\n",
        "        mu = 0\n",
        "        sigma = 6 \n",
        "    else:\n",
        "        mu = 0\n",
        "        sigma = 0.1\n",
        "        \n",
        "    n_samples, n_features = ds[omic].shape\n",
        "\n",
        "    # Add salt & pepper noise\n",
        "    np.random.seed(42)   # fixed seed for reproducibility\n",
        "    noise = np.random.normal(mu, sigma, size=(n_samples, n_features))\n",
        "    p = 0.5 #0.8     # salt & pepper noise: add (gaussian) noise to a particolar feature of a particular sample with probability p\n",
        "    raveled_indices = np.random.choice(np.arange(n_samples*n_features), replace=False, size=int(n_samples*n_features*(1-p)))\n",
        "    indices = np.unravel_index(raveled_indices, (n_samples, n_features))\n",
        "    noise[indices] = 0\n",
        "\n",
        "    print(f\"Salt & pepper gaussian noise N({mu},{sigma**2}) is added to the {omic} dataset\")\n",
        "    ds[f'{omic}_noisy'] = ds[f'{omic}'] + noise\n",
        "\n",
        "    # Normalize omic\n",
        "    ds[f'{omic}_noisy'] = MinMaxScaler().fit_transform(ds[f'{omic}_noisy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCSv87IxVQ33"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBS05EGwOSFF"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_noisy'])\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ly2_2mVJ1W"
      },
      "source": [
        "Early integration + K-means (For comparison)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cdmKFknRY0x"
      },
      "source": [
        "# Concatenate the omics (early integration)\n",
        "ds['early_integr'] = np.concatenate([ds[f'{omic}_noisy'] for omic in omics], axis=1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(ds['early_integr'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "principalComponents = pca.fit_transform(ds['early_integr'])\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "print(f\"Silhouette, 2 clusters: {silhouette_score(ds['early_integr'], cluster_labels)}\")\n",
        "\n",
        "# Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "plot_2D_dataset(principalComponents, cluster_labels, cluster_centers=kmeans.cluster_centers_, title=f'Early integrated dataset visualization', caption='2 clusters')\n",
        "plot_2D_dataset(principalComponents, true_cluster_labels, title=f'Early integrated dataset visualization', caption='true clusters')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(true_cluster_labels, cluster_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVgQmI3OSFG"
      },
      "source": [
        "## Integration method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpasLCmxOSFG"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_noisy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WScMPTR4OSFG"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6A95J5KOSFG"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbXBPPmAju3W"
      },
      "source": [
        "# Kidney dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAJmxQ9Vn00W"
      },
      "source": [
        "# Upload on Google Colab, autoencoder for each omic \n",
        "ds_ae = {}     # this will contain each autoencoder trained on each omic\n",
        "\n",
        "# Load autoencoders\n",
        "if not os.path.exists(\"./autoencoder_mRNA_kidney.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1Q3Dw-x_6lyXqmxJoHojBbrYRcgibtIkb' -O ./autoencoder_mRNA_kidney.zip\n",
        "if not os.path.exists(\"./autoencoder_miRNA_kidney.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1eo2ZKjEsmA7_mNQhP9m0mw3kIqyjazEy' -O ./autoencoder_miRNA_kidney.zip\n",
        "if not os.path.exists(\"./autoencoder_meth_kidney.zip\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=15RXyvNiovuG_G0Su2N0LuoqtNTYlqWdA' -O ./autoencoder_meth_kidney.zip\n",
        "\n",
        "!unzip autoencoder_mRNA_kidney.zip\n",
        "ds_ae[\"mRNA\"] = tf.keras.models.load_model('autoencoder_mRNA_kidney.tf')\n",
        "!unzip autoencoder_miRNA_kidney.zip\n",
        "ds_ae[\"miRNA\"] = tf.keras.models.load_model('autoencoder_miRNA_kidney.tf')\n",
        "!unzip autoencoder_meth_kidney.zip\n",
        "ds_ae[\"meth\"] = tf.keras.models.load_model('autoencoder_meth_kidney.tf')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ap72hnIju-G"
      },
      "source": [
        "# Load dataset\n",
        "if not os.path.exists(\"./mRNA_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1i1do_UTzwXzPVIDDmYSFJEholK2Mp8g_' -O ./mRNA_kidney.txt\n",
        "if not os.path.exists(\"./miRNA_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1liKeOBKjnbCi1CIjcOPA3Zxv2fRzCfa2' -O ./miRNA_kidney.txt\n",
        "if not os.path.exists(\"./meth_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1qr9joY0bAVDLvjWsKF5xf3CaRolBu-mP' -O ./meth_kidney.txt\n",
        "if not os.path.exists(\"./clusters_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1R-U2iDgM4oEyzNRfBIA2kXMbKw_s0QtI' -O ./clusters_kidney.txt\n",
        "\n",
        "ds = {}     # this will contain each omic\n",
        "omics = ['mRNA','miRNA','meth']\n",
        "for omic_name in omics:\n",
        "    path = omic_name + \"_kidney.txt\"\n",
        "    if omic_name not in ds:\n",
        "        ds[omic_name] = pd.read_csv(path, sep='\\t', index_col=0)\n",
        "\n",
        "\n",
        "y = pd.read_csv('clusters_kidney.txt', sep='\\t', index_col=0)\n",
        "true_cluster_labels = y.values.reshape(y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJR_B1kGwlZ3"
      },
      "source": [
        "### mRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzjHdluawkSL"
      },
      "source": [
        "# Keep only protein coding genes\n",
        "# NB: the file idx_mRNA_prot_cod.txt, containing the protein coding genes of the\n",
        "# human genome, has been obtained in the colab notebook 'Find protein coding genes'\n",
        "if not os.path.exists(\"./idx_mRNA_prot_cod_kidney.txt\"):\n",
        "    !gdown 'https://drive.google.com/uc?id=1Pi4u8y_YAc2tmOWZYaeLn9wGdzu4cFC5' -O ./idx_mRNA_prot_cod_kidney.txt\n",
        "\n",
        "idx_mRNA_prot_cod = pd.read_csv('idx_mRNA_prot_cod_kidney.txt')\n",
        "idx_mRNA_prot_cod = idx_mRNA_prot_cod['idx'].values\n",
        "\n",
        "ds['mRNA'] = ds['mRNA'].iloc[:, idx_mRNA_prot_cod]\n",
        "\n",
        "# Delete genes with a zero expression value across all the samples\n",
        "ds['mRNA'] = ds['mRNA'].loc[:, (ds['mRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize mRNA with MinMax Scaler\n",
        "ds['mRNA_normalized'] = MinMaxScaler().fit_transform(ds['mRNA'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYv31QjyxjpO"
      },
      "source": [
        "### miRNA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7aH-tn7w1zQ"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['miRNA'] = ds['miRNA'].loc[:, (ds['miRNA'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with log2 normalization\n",
        "ds['miRNA'] = np.log(ds['miRNA'] + 1) / np.log(2)\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['miRNA_normalized'] = MinMaxScaler().fit_transform(ds['miRNA'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cdalav4yhbe"
      },
      "source": [
        "### meth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ik5RM1Mytf5"
      },
      "source": [
        "# Delete sequences with a zero expression value across all the samples\n",
        "ds['meth'] = ds['meth'].loc[:, (ds['meth'] != 0).any(axis=0)]\n",
        "\n",
        "# Normalize with MinMaxScaler\n",
        "ds['meth_normalized'] = MinMaxScaler().fit_transform(ds['meth'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vvkwM9JVqLL"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jyl7EMk1M-CQ"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_normalized'])\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MqjrL0in854"
      },
      "source": [
        "## Integration method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1FMzf1_tntk"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_normalized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "984XUbfEn8AH"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUjEflfKvTAd"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6HyNaPCVEDH"
      },
      "source": [
        "# Kidney dataset (w/ noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfZB9V3rp8hL"
      },
      "source": [
        "We can verify that when introducing some salt & pepper noise, our method still performs well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPSEBoltqeQm"
      },
      "source": [
        "for omic in omics:\n",
        "    if omic == \"mRNA\":\n",
        "        mu = 0 \n",
        "        sigma = 5\n",
        "\n",
        "    elif omic == \"miRNA\":\n",
        "        mu = 0\n",
        "        sigma = 7\n",
        "\n",
        "    elif omic == \"meth\":\n",
        "        mu = 0\n",
        "        sigma = 1\n",
        "        \n",
        "    n_samples, n_features = ds[omic].shape\n",
        "\n",
        "    # Add salt & pepper noise\n",
        "    np.random.seed(42)   # fixed seed for reproducibility\n",
        "    noise = np.random.normal(mu, sigma, size=(n_samples, n_features))\n",
        "    p = 0.5 #0.8     # salt & pepper noise: add (gaussian) noise to a particolar feature of a particular sample with probability p\n",
        "    raveled_indices = np.random.choice(np.arange(n_samples*n_features), replace=False, size=int(n_samples*n_features*(1-p)))\n",
        "    indices = np.unravel_index(raveled_indices, (n_samples, n_features))\n",
        "    noise[indices] = 0\n",
        "\n",
        "    print(f\"Salt & pepper gaussian noise N({mu},{sigma**2}) is added to the {omic} dataset\")\n",
        "    ds[f'{omic}_noisy'] = ds[f'{omic}'] + noise\n",
        "\n",
        "    # Normalize omic\n",
        "    ds[f'{omic}_noisy'] = MinMaxScaler().fit_transform(ds[f'{omic}_noisy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjq0XkKEVvvB"
      },
      "source": [
        "Omic representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-06bJdOVvvC"
      },
      "source": [
        "for omic in omics:\n",
        "    pca = PCA(n_components=2)\n",
        "    pc2 = pca.fit_transform(ds[f'{omic}_noisy'])\n",
        "    plot_2D_dataset(pc2,true_cluster_labels,title=f'{omic} 2D dataset visualization',caption='True labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIOAfamZWKFn"
      },
      "source": [
        "Early integration + K-means (For comparison)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWf-mOQkWKFo"
      },
      "source": [
        "# Concatenate the omics (early integration)\n",
        "ds['early_integr'] = np.concatenate([ds[f'{omic}_noisy'] for omic in omics], axis=1)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "cluster_labels = kmeans.fit_predict(ds['early_integr'])\n",
        "\n",
        "# Perform a 2D PCA to visualize the dataset\n",
        "pca = PCA(2)\n",
        "principalComponents = pca.fit_transform(ds['early_integr'])\n",
        "kmeans.cluster_centers_ = pca.transform(kmeans.cluster_centers_)\n",
        "\n",
        "# Compute silhouette on the original dataset with cluster assignments and true cluster labels\n",
        "print(f\"Silhouette, 3 clusters: {silhouette_score(ds['early_integr'], cluster_labels)}\")\n",
        "\n",
        "# Plot the clustered dataset with cluster assignments and true cluster labels\n",
        "plot_2D_dataset(principalComponents, cluster_labels, cluster_centers=kmeans.cluster_centers_, title=f'Early integrated dataset visualization', caption='3 clusters')\n",
        "plot_2D_dataset(principalComponents, true_cluster_labels, title=f'Early integrated dataset visualization', caption='true clusters')\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plot_confusion_matrix(true_cluster_labels, cluster_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-briqdrVvvD"
      },
      "source": [
        "## Integration method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDsk0XqNVvvD"
      },
      "source": [
        "# Find the latent space for each omic\n",
        "ds_encoded = {}\n",
        "for omic_name in omics:\n",
        "    if omic_name not in ds_encoded:\n",
        "        _, ds_encoded[omic_name] = ds_ae[omic_name](ds[f\"{omic_name}_noisy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3C2Of9VVvvD"
      },
      "source": [
        "# Compute the distance matrix for each encoded omic (and then average them together)\n",
        "dist_mat = []\n",
        "for omic in omics:\n",
        "    dist_mat.append(distance_matrix(ds_encoded[omic], ds_encoded[omic]))\n",
        "dist_mat = np.stack(dist_mat)\n",
        "\n",
        "avg_dist_matrix = np.average(dist_mat, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t-GGKILVvvE"
      },
      "source": [
        "#####\n",
        "# Cluster with spectral clustering\n",
        "\n",
        "embedding = MDS(n_components=2, dissimilarity='precomputed',random_state=0)\n",
        "pc2 = embedding.fit_transform(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, true_cluster_labels, title='Integrated dataset visualization', caption='True labels')\n",
        "\n",
        "#####\n",
        "# Results for the avg integration\n",
        "best_K = 0\n",
        "best_silh= -1\n",
        "\n",
        "MAX_CLUSTERS = 10\n",
        "for K in range(2, MAX_CLUSTERS+1):\n",
        "    spectral = SpectralClustering(n_clusters=K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "    cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "    silh = silhouette_score(avg_dist_matrix, cluster_assignments, metric=\"precomputed\")\n",
        "    if silh > best_silh:\n",
        "        best_silh = silh\n",
        "        best_K = K\n",
        "\n",
        "    # Visualize clustering results and conf mat\n",
        "    print(f\"----- {K} CLUSTERS -----\")\n",
        "    print(f\"silhouette: {silh}\")\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "# Best result\n",
        "print(f'Best K found (standard avg integration): {best_K}')\n",
        "spectral = SpectralClustering(n_clusters=best_K, affinity='precomputed_nearest_neighbors', n_neighbors=8) # random_state=0\n",
        "cluster_assignments = spectral.fit_predict(avg_dist_matrix)\n",
        "plot_2D_dataset(pc2, cluster_assignments, title='Integrated dataset visualization', caption=f'{best_K} clusters, silhouette = {best_silh:.4f}')\n",
        "plot_confusion_matrix(true_cluster_labels,cluster_assignments, title=f'Confusion matrix of the cluster labels, K={best_K}')\n",
        "print()\n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
